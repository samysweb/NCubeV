{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will train with a modified reward scheme..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`polytope` failed to import `cvxopt.glpk`.\n",
      "will use `scipy.optimize.linprog`\n"
     ]
    }
   ],
   "source": [
    "import zeppelin_gym.env6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f358c3e85d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/anaconda3/envs/nnv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('zeppelin-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.FUEL_RESTRAINT = False\n",
    "env.unwrapped.OBSTACLE_REWARD = -1.\n",
    "env.unwrapped.NO_FUEL_REWARD = -0.1\n",
    "# done reward = (FUEL_RESTRAINT) ? r+fuel*r : 2*r\n",
    "env.unwrapped.DONE_REWARD = 0.5\n",
    "env.unwrapped.INCLUDE_UNWINNABLE = False\n",
    "env.unwrapped.EMERGENCY_REWARD = -1e-3\n",
    "env.unwrapped.WORST_CASE_TURBULENCE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "architecture = [dict(pi=[8,8], vf=[8,8])]\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,policy_kwargs={\"activation_fn\":nn.ReLU,\"net_arch\":architecture})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel/anaconda3/envs/nnv/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-14.20 +/- 78.44\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1000)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,total_timesteps,num_iterations,timesteps_per_iter,eval_steps,model_name):\n",
    "    print(\"Mean Reward \\t\\t Timesteps \\t\\t Seconds (iter)\")\n",
    "    for i in range(num_iterations):\n",
    "        total_timesteps += timesteps_per_iter\n",
    "        start_time = time.time()\n",
    "        model=model.learn(total_timesteps=timesteps_per_iter,log_interval=10000)\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=eval_steps)\n",
    "        model.save(model_name+str(total_timesteps))\n",
    "        print(f\"{mean_reward:.2f} +/- {std_reward:.2f}\",end=\"\\t\\t\")\n",
    "        print(total_timesteps,end=\"\\t\\t\")\n",
    "        print(f\"{(time.time() - start_time):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward \t\t Timesteps \t\t Seconds (iter)\n",
      "-4.27 +/- 21.55\t\t100000\t\t85.78s\n",
      "-3.59 +/- 21.50\t\t200000\t\t81.37s\n",
      "-2.85 +/- 13.07\t\t300000\t\t79.86s\n",
      "-3.65 +/- 25.38\t\t400000\t\t78.35s\n",
      "-2.27 +/- 5.03\t\t500000\t\t73.92s\n",
      "-2.50 +/- 5.93\t\t600000\t\t72.60s\n",
      "-1.93 +/- 4.56\t\t700000\t\t73.21s\n",
      "-2.06 +/- 5.49\t\t800000\t\t72.58s\n",
      "-1.65 +/- 3.55\t\t900000\t\t71.88s\n",
      "-2.42 +/- 5.59\t\t1000000\t\t71.00s\n"
     ]
    }
   ],
   "source": [
    "train(model,0,10,100_000,1_000,\"model_backup/zeppelin-v2.2-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward \t\t Timesteps \t\t Seconds (iter)\n",
      "-2.20 +/- 5.54\t\t1100000\t\t72.05s\n",
      "-2.30 +/- 5.41\t\t1200000\t\t72.17s\n",
      "-2.02 +/- 4.48\t\t1300000\t\t72.37s\n",
      "-1.93 +/- 4.65\t\t1400000\t\t69.43s\n",
      "-2.48 +/- 8.39\t\t1500000\t\t70.19s\n",
      "-2.30 +/- 10.06\t\t1600000\t\t72.48s\n",
      "-2.85 +/- 14.72\t\t1700000\t\t71.82s\n",
      "-2.92 +/- 20.43\t\t1800000\t\t71.12s\n",
      "-2.04 +/- 5.50\t\t1900000\t\t71.08s\n",
      "-1.73 +/- 4.22\t\t2000000\t\t70.65s\n"
     ]
    }
   ],
   "source": [
    "train(model,1_000_000,10,100_000,1_000,\"model_backup/zeppelin-v2.2-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward \t\t Timesteps \t\t Seconds (iter)\n",
      "-2.17 +/- 5.69\t\t2100000\t\t71.16s\n",
      "-2.01 +/- 5.61\t\t2200000\t\t69.59s\n",
      "-2.65 +/- 14.51\t\t2300000\t\t70.38s\n",
      "-2.26 +/- 5.80\t\t2400000\t\t72.41s\n",
      "-1.96 +/- 5.04\t\t2500000\t\t69.51s\n",
      "-2.14 +/- 5.61\t\t2600000\t\t68.85s\n",
      "-2.21 +/- 11.23\t\t2700000\t\t70.16s\n",
      "-2.03 +/- 5.62\t\t2800000\t\t69.63s\n",
      "-2.62 +/- 7.81\t\t2900000\t\t70.19s\n",
      "-2.68 +/- 6.81\t\t3000000\t\t69.82s\n"
     ]
    }
   ],
   "source": [
    "train(model,2_000_000,10,100_000,1_000,\"model_backup/zeppelin-v2.2-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward \t\t Timesteps \t\t Seconds (iter)\n",
      "-3.63 +/- 28.65\t\t3100000\t\t71.79s\n",
      "-3.68 +/- 16.08\t\t3200000\t\t74.34s\n",
      "-3.01 +/- 20.31\t\t3300000\t\t72.73s\n",
      "-2.16 +/- 5.67\t\t3400000\t\t72.16s\n",
      "-1.64 +/- 4.47\t\t3500000\t\t71.80s\n",
      "-1.59 +/- 4.28\t\t3600000\t\t70.78s\n",
      "-1.68 +/- 4.76\t\t3700000\t\t69.87s\n",
      "-1.72 +/- 4.79\t\t3800000\t\t70.42s\n",
      "-1.75 +/- 5.15\t\t3900000\t\t70.21s\n",
      "-1.67 +/- 4.44\t\t4000000\t\t70.37s\n"
     ]
    }
   ],
   "source": [
    "train(model,3_000_000,10,100_000,1_000,\"model_backup/zeppelin-v2.2-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward \t\t Timesteps \t\t Seconds (iter)\n",
      "-2.37 +/- 12.81\t\t4100000\t\t70.68s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_backup/zeppelin-v2.2-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, total_timesteps, num_iterations, timesteps_per_iter, eval_steps, model_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mtimesteps_per_iter,log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(total_timesteps))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nnv/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:87\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     86\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(observations, state\u001b[38;5;241m=\u001b[39mstates, episode_start\u001b[38;5;241m=\u001b[39mepisode_starts, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[0;32m---> 87\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     89\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nnv/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nnv/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/anaconda3/envs/nnv/lib/python3.8/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Dokumente/Projects/NN-Safety-via-dL/repos/NCubeV/experiments/zeppelin/training/zeppelin_gym/env6.py:211\u001b[0m, in \u001b[0;36mZeppelinEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stepByModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dokumente/Projects/NN-Safety-via-dL/repos/NCubeV/experiments/zeppelin/training/zeppelin_gym/env6.py:289\u001b[0m, in \u001b[0;36mZeppelinEnv._stepByModel\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    286\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEMERGENCY_REWARD\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 289\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m0.\u001b[39m,\u001b[43mquant_modelplex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantitative_modelplex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIME_STEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_VELOCITY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_TURBULENCE\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10.0\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m#print((x1, x2, c, w, y1_norm, y2_norm, self.TIME_STEP, self.MAX_WIND_SPEED, self.MAX_TURBULENCE))\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m#print(\"modelplex:\",quant_modelplex.quantitative_modelplex(x1, x2, c, w, y1_norm, y2_norm, self.TIME_STEP, self.MAX_WIND_SPEED, self.MAX_TURBULENCE))\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate), reward, done, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrash\u001b[39m\u001b[38;5;124m'\u001b[39m: has_crashed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m'\u001b[39m: reached_goal}\n",
      "File \u001b[0;32m~/Dokumente/Projects/NN-Safety-via-dL/repos/NCubeV/experiments/zeppelin/training/zeppelin_gym/quant_modelplex.py:105\u001b[0m, in \u001b[0;36mquantitative_modelplex\u001b[0;34m(x1, x2, c, w, y1post, y2post, T, p, r)\u001b[0m\n\u001b[1;32m     11\u001b[0m z1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m z2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m     15\u001b[0m     \n\u001b[1;32m     16\u001b[0m         y1post\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m+\u001b[39m y2post\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     18\u001b[0m         \n\u001b[1;32m     19\u001b[0m             \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     20\u001b[0m             \n\u001b[1;32m     21\u001b[0m                 x2 \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m-\u001b[39mc2 \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m (p \u001b[38;5;241m+\u001b[39m r)),\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     23\u001b[0m                 \n\u001b[1;32m     24\u001b[0m                     c2 \u001b[38;5;241m+\u001b[39m w \u001b[38;5;241m/\u001b[39m (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m*\u001b[39m c1 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m (p \u001b[38;5;241m+\u001b[39m r \u001b[38;5;241m+\u001b[39m w) \u001b[38;5;241m-\u001b[39m x2,\n\u001b[1;32m     25\u001b[0m                     \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     26\u001b[0m                     \n\u001b[1;32m     27\u001b[0m                         (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m w \u001b[38;5;241m*\u001b[39m (c2 \u001b[38;5;241m-\u001b[39m (x2 \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m (r \u001b[38;5;241m+\u001b[39m p \u001b[38;5;241m+\u001b[39m w))) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     28\u001b[0m                         c1 \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m     29\u001b[0m                         (x1 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m (p \u001b[38;5;241m+\u001b[39m r)) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     30\u001b[0m                         (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m w \u001b[38;5;241m*\u001b[39m (c2 \u001b[38;5;241m-\u001b[39m (x2 \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m (r \u001b[38;5;241m+\u001b[39m p \u001b[38;5;241m+\u001b[39m w))) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     31\u001b[0m                         c1 \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m     32\u001b[0m                         \u001b[38;5;241m-\u001b[39m(x1 \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m (p \u001b[38;5;241m+\u001b[39m r)) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     33\u001b[0m                         \n\u001b[1;32m     34\u001b[0m                     )\n\u001b[1;32m     35\u001b[0m                     \n\u001b[1;32m     36\u001b[0m                 )\n\u001b[1;32m     37\u001b[0m                 \n\u001b[1;32m     38\u001b[0m             ),\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     40\u001b[0m             \n\u001b[1;32m     41\u001b[0m                 \u001b[38;5;28mmax\u001b[39m(x2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m-\u001b[39mc2,x2 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m p \u001b[38;5;241m*\u001b[39m y2post \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m-\u001b[39mc2),\n\u001b[1;32m     42\u001b[0m                 \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     43\u001b[0m                 \n\u001b[1;32m     44\u001b[0m                     \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m     45\u001b[0m                     \n\u001b[1;32m     46\u001b[0m                         c2 \u001b[38;5;241m+\u001b[39m w \u001b[38;5;241m/\u001b[39m (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m*\u001b[39m c1 \u001b[38;5;241m-\u001b[39m x2,\n\u001b[1;32m     47\u001b[0m                         c2 \u001b[38;5;241m+\u001b[39m w \u001b[38;5;241m/\u001b[39m (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m*\u001b[39m c1 \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m     48\u001b[0m                         (x2 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m p \u001b[38;5;241m*\u001b[39m y2post \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m r)\n\u001b[1;32m     49\u001b[0m                         \n\u001b[1;32m     50\u001b[0m                     ),\n\u001b[1;32m     51\u001b[0m                     \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m     52\u001b[0m                     \n\u001b[1;32m     53\u001b[0m                         \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m     54\u001b[0m                         \n\u001b[1;32m     55\u001b[0m                             (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m w \u001b[38;5;241m*\u001b[39m (c2 \u001b[38;5;241m-\u001b[39m x2) \u001b[38;5;241m+\u001b[39m c1 \u001b[38;5;241m-\u001b[39m x1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     56\u001b[0m                             (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m w \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     57\u001b[0m                             (\n\u001b[1;32m     58\u001b[0m                             c2 \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m     59\u001b[0m                             (\n\u001b[1;32m     60\u001b[0m                                 x2 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m p \u001b[38;5;241m*\u001b[39m y2post \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     61\u001b[0m                                 T \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     62\u001b[0m                                 (\u001b[38;5;241m-\u001b[39m(p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m (w\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m+\u001b[39m (p \u001b[38;5;241m-\u001b[39m r)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[1;32m     63\u001b[0m                             )\n\u001b[1;32m     64\u001b[0m                             ) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     65\u001b[0m                             c1 \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m     66\u001b[0m                             (\n\u001b[1;32m     67\u001b[0m                             x1 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m p \u001b[38;5;241m*\u001b[39m y1post \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     68\u001b[0m                             T \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m*\u001b[39m (w \u001b[38;5;241m/\u001b[39m (w\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m+\u001b[39m (p \u001b[38;5;241m-\u001b[39m r)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[1;32m     69\u001b[0m                             ) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     70\u001b[0m                             (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     71\u001b[0m                             \n\u001b[1;32m     72\u001b[0m                         ),\n\u001b[1;32m     73\u001b[0m                         \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m     74\u001b[0m                         \n\u001b[1;32m     75\u001b[0m                             (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m w \u001b[38;5;241m*\u001b[39m (c2 \u001b[38;5;241m-\u001b[39m x2) \u001b[38;5;241m+\u001b[39m c1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m-\u001b[39mx1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[1;32m     76\u001b[0m                             (p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m w \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     77\u001b[0m                             (\n\u001b[1;32m     78\u001b[0m                             c2 \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m     79\u001b[0m                             (\n\u001b[1;32m     80\u001b[0m                                 x2 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m p \u001b[38;5;241m*\u001b[39m y2post \u001b[38;5;241m-\u001b[39m T \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     81\u001b[0m                                 T \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     82\u001b[0m                                 (\u001b[38;5;241m-\u001b[39m(p \u001b[38;5;241m-\u001b[39m r) \u001b[38;5;241m/\u001b[39m (w\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m+\u001b[39m (p \u001b[38;5;241m-\u001b[39m r)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[1;32m     83\u001b[0m                             )\n\u001b[1;32m     84\u001b[0m                             ) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     85\u001b[0m                             c1 \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m     86\u001b[0m                             \u001b[38;5;241m-\u001b[39m(\n\u001b[1;32m     87\u001b[0m                             x1 \u001b[38;5;241m+\u001b[39m T \u001b[38;5;241m*\u001b[39m p \u001b[38;5;241m*\u001b[39m y1post \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     88\u001b[0m                             T \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39mw \u001b[38;5;241m/\u001b[39m (w\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m+\u001b[39m (p \u001b[38;5;241m-\u001b[39m r)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[1;32m     89\u001b[0m                             ) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     90\u001b[0m                             (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     91\u001b[0m                             \n\u001b[1;32m     92\u001b[0m                         )\n\u001b[1;32m     93\u001b[0m                         \n\u001b[1;32m     94\u001b[0m                     )\n\u001b[1;32m     95\u001b[0m                     \n\u001b[1;32m     96\u001b[0m                 )\n\u001b[1;32m     97\u001b[0m                 \n\u001b[1;32m     98\u001b[0m             )\n\u001b[1;32m     99\u001b[0m             \n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    101\u001b[0m         \n\u001b[1;32m    102\u001b[0m     ),\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtpost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpost\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    107\u001b[0m         \n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mmax\u001b[39m(x1post \u001b[38;5;241m-\u001b[39m x1,x1 \u001b[38;5;241m-\u001b[39m x1post),\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    110\u001b[0m             \n\u001b[1;32m    111\u001b[0m                 \u001b[38;5;28mmax\u001b[39m(x2post \u001b[38;5;241m-\u001b[39m x2,x2 \u001b[38;5;241m-\u001b[39m x2post),\n\u001b[1;32m    112\u001b[0m                 \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m                 \n\u001b[1;32m    114\u001b[0m                     \u001b[38;5;28mmax\u001b[39m(z1post \u001b[38;5;241m-\u001b[39m z1,z1 \u001b[38;5;241m-\u001b[39m z1post),\n\u001b[1;32m    115\u001b[0m                     \u001b[38;5;28mmax\u001b[39m(z2post \u001b[38;5;241m-\u001b[39m z2,z2 \u001b[38;5;241m-\u001b[39m z2post)\n\u001b[1;32m    116\u001b[0m                     \n\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m                 \n\u001b[1;32m    119\u001b[0m             )\n\u001b[1;32m    120\u001b[0m             \n\u001b[1;32m    121\u001b[0m         )\n\u001b[1;32m    122\u001b[0m         \n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \n\u001b[1;32m    125\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,4_000_000,10,100_000,1_000,\"model_backup/zeppelin-v2.2-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,5_000_000,10,100_000,1_000,\"model_backup/zeppelin-v2.2-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(43)\n",
    "torch.manual_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"model_backup/zeppelin-v2.2-1000000\")\n",
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"zeppelin-avoidance-windsystem-small2-1400000\")\n",
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"hi\")\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    env.unwrapped.state=[-5,110,50,22]\n",
    "    total_reward=0\n",
    "    for t in range(1000):\n",
    "        action, _states = model.predict(observation)\n",
    "        #x1_norm = observation[0]/np.sqrt(observation[0]**2+observation[1]**2)\n",
    "        #x2_norm = observation[1]/np.sqrt(observation[0]**2+observation[1]**2)\n",
    "        #f = 1.0 if (x2_norm) < 0 else -1.0\n",
    "        #action = [f, f*(0-x1_norm)]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(observation)\n",
    "        total_reward=0.99*total_reward+reward\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(\"Reward: \", total_reward)\n",
    "            time.sleep(5.0)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "env.seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.WORST_CASE_TURBULENCE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30000)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.WORST_CASE_TURBULENCE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30000)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "env.seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.WORST_CASE_TURBULENCE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30000)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.WORST_CASE_TURBULENCE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30000)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"model_backup/zeppelin-v2.2-4000000\")\n",
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "env.seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal:  376\n",
      "Obstacle:  1930\n",
      "Harmless:  7694\n"
     ]
    }
   ],
   "source": [
    "reached_goal = 0\n",
    "reached_obstacle = 0\n",
    "ended_harmless = 0\n",
    "for i_episode in range(10000):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        action, _states = model.predict(observation,deterministic=True)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            if env.unwrapped.reached_goal(observation):\n",
    "                reached_goal+=1\n",
    "            elif env.unwrapped.is_crash(observation):\n",
    "                reached_obstacle +=1\n",
    "            else:\n",
    "                ended_harmless +=1\n",
    "            break\n",
    "print(\"Goal: \", reached_goal)\n",
    "print(\"Obstacle: \", reached_obstacle)\n",
    "print(\"Harmless: \", ended_harmless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11.719801522828476, -18.12381758102127, 18.056895293282267, 23.814626016359313, 234.05477516465498, 143.73660312372135)\n",
      "[  17.79752 -407.89762   18.0569    23.81463  234.05478  143.7366 ]\n",
      "Episode finished after 23 timesteps (3 emergencies)\n",
      "Reward:  -0.1024294243338249\n",
      "(24.705427011254347, 39.19610885480948, 15.480028053639526, 23.475582890012316, -254.71534512017024, 30.347523750409266)\n",
      "[   0.18013    6.8427    15.48003   23.47558 -254.71535   30.34752]\n",
      "Episode finished after 3 timesteps (0 emergencies)\n",
      "Reward:  -1.1417363631662678\n",
      "(131.57939351776594, -5.479940720388441, 76.42979802076961, 23.860503281689684, 384.0554270546447, -7.88102947339604)\n",
      "[ 173.63655 -407.90534   76.4298    23.8605   384.05543   -7.88103]\n",
      "Episode finished after 27 timesteps (10 emergencies)\n",
      "Reward:  -0.10806018371963638\n",
      "(-6.0692463456440535, 42.850342928726896, 17.61353349131152, 17.97190516261096, 374.34657039035187, -364.31384759390653)\n",
      "[ -45.81047 -400.90741   17.61353   17.97191  374.34657 -364.31385]\n",
      "Episode finished after 37 timesteps (24 emergencies)\n",
      "Reward:  -0.11880729759420283\n",
      "(-17.56931711796212, 136.01493096987213, 46.34536577709751, 20.566621576550986, 317.0590613638028, 186.53134946428577)\n",
      "[-115.21817 -402.59102   46.34537   20.56662  317.05906  186.53135]\n",
      "Episode finished after 46 timesteps (27 emergencies)\n",
      "Reward:  -0.1196346505715303\n",
      "(35.96560885856343, 3.1311948963181564, 14.12212700887673, 16.15341345029288, -117.23736459448253, 345.16291248579034)\n",
      "[  58.77674 -411.85968   14.12213   16.15341 -117.23736  345.16291]\n",
      "Episode finished after 34 timesteps (14 emergencies)\n",
      "Reward:  -0.1107355224260545\n",
      "(9.381778670234848, 183.17617637211256, 44.56040045626788, 27.270859254343122, -101.30874296214307, -390.96809146838547)\n",
      "[  12.26268   33.07892   44.5604    27.27086 -101.30874 -390.96809]\n",
      "Episode finished after 8 timesteps (0 emergencies)\n",
      "Reward:  -9.805715894324717\n",
      "(-21.6478483956915, 18.577626630238495, 11.115613380795363, 10.275989626997239, -337.23733037424273, -20.971929215200362)\n",
      "[-306.54875  -13.9552    11.11561   10.27599 -337.23733  -20.97193]\n",
      "Episode finished after 31 timesteps (0 emergencies)\n",
      "Reward:  0.9997423033696544\n",
      "(89.66523318641467, -8.665070217886004, 41.83350124176498, 16.62046249174399, -35.846682968219284, -339.49975590616646)\n",
      "[  99.87758 -406.04366   41.8335    16.62046  -35.84668 -339.49976]\n",
      "Episode finished after 29 timesteps (0 emergencies)\n",
      "Reward:  -0.9289806661617984\n",
      "(48.764310350967996, 65.41423310855177, 46.111817131987124, 12.657679174611419, 18.708196967510275, 145.2869751077851)\n",
      "[ 193.84193 -411.35903   46.11182   12.65768   18.7082   145.28698]\n",
      "Episode finished after 67 timesteps (40 emergencies)\n",
      "Reward:  -0.12523590147274427\n"
     ]
    }
   ],
   "source": [
    "env.unwrapped.INCLUDE_UNWINNABLE = False\n",
    "#env.init_polytopes(0.0,retrain_polytopes_certain)\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    #env.unwrapped.state = (-200.52995,  -49.5127 ,   72.78154 ,  16.0011   , -200.83888,  -357.2502)\n",
    "    observation = env.unwrapped.state\n",
    "    total_reward=0\n",
    "    emergencies=0\n",
    "    print(observation)\n",
    "    for t in range(1000):\n",
    "        x1,x2,c,w,g1,g2 = observation\n",
    "        #action = [0.,0.,0.0]\n",
    "        \n",
    "        #action=[0.,0.,1.]\n",
    "        action, _states = model.predict(observation,deterministic=True)\n",
    "        if action[2] > env.unwrapped.EMERGENCY_THRESHOLD:\n",
    "            emergencies+=1\n",
    "        #print([action[1]*action[0],action[1]*np.sqrt(1-action[0]**2)])\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #assert reward >= 0, f\"reward: {reward}\"\n",
    "        #print(observation)\n",
    "        #print(reward)\n",
    "        total_reward=0.99*total_reward+reward\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "        if done:\n",
    "            print(observation)\n",
    "            print(\"Episode finished after {} timesteps ({} emergencies)\".format(t+1,emergencies))\n",
    "            print(\"Reward: \", total_reward)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
